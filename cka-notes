Note : 
Set auto-completion for aliases
complete -p YOUR_CMD # find the command fucntion
complete -F YOUR_CMD_FUNCTION kubectl # set this in bashrc

WARNING : FAIRE UN FOCUS SUR 
- LES API-GATEWAY
- LES STRATEGY DANS LES DEPLOYMENT
- KUSTOMIZE
- HELM
- YQ

## RBAC :
Subject: The user or process that wants to access a resource.
Resource: The Kubernetes API resource type e.g. a Deployment or node.
Verb: The operation that can be ex
kubectl create role <name> --verb=<verbs> --resource=<resources> -n <ns>
kubectl create rolebinding <name> --role=<role> --user=<user> -n <ns>
kubectl create clusterrole ...
kubectl create clusterrolebinding ...
kubectl auth can-i <verb> <resource> --as <user>


# create a user in k8s : https://kubernetes.io/docs/tasks/tls/certificate-issue-client-csr
# change the default ns
kubectl config set-context --current --namespace=NAMESPACE

# YAML syntax
microservice : #objet
  app : auth # key : value
  port : 300
  version : 1.7

# In case of list :

microservice :
  - app : auth
    port : 300
    version : 1.7
  - app: user
    port : 3009
    version : 1.8

# paste un multiline dans un yaml file
mylines: |
   line 1
   line 2
   line 3

Exemple executer un script 
command: 
   - sh
   - -C 
   - |
    Paste all 
    Your script 

#Paste a long sentence in  yaml
mylongsentence: >
    Mariame joue
    a la balle 
    avec bile 
    son ami

# check permission
kubectl auth can-i --list --as johndoe
kubectl auth can-i list pods --as johndoe

# Update iptables config
cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sudo sysctl --system

# Backup and restore ETCD
### Backup
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=<trusted-ca-file> --cert=<cert-file> --key=<key-file> \
  snapshot save <backup-file-location>

Exemple : 
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /tmp/snapshot.db

#### Restore
export ETCDCTL_API=3
etcdctl --data-dir <data-dir-location> snapshot restore snapshot.db

Now edit /etc/kubernetes/manifests/etcd.yaml and change **volumes.hostPath.path** for name: etcd-data to <data-dir-location>,
then execute kubectl -n kube-system delete pod <name-of-etcd-pod> or systemctl restart kubelet.service or both

# Determine the api-server endpoint
kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}'

# Get the Secret access token of a ServiceAccount
kubectl get secret $(kubectl get serviceaccount api-access -n apps \
 -o jsonpath='{.secrets[0].name}') -o jsonpath='{.data.token}' -n apps \
 | base64 --decode

# Curl for listing pods in ns rm from a pod
curl https://10.10.10.38:6443/api/v1/namespaces/rm/pods --header --insecure

#  Rolling Updates and Rollbacks 
### Rolling update
kubectl set image deployment app-cache memcached=memcached:1.6.10 --record

### Check a rollout status
k rollout status deployment app-cache

### Check rollout history
k rollout history deployment app-cache

### Get more details about a rollout
k rollout history deployment app-cache --revision=2

### Rolling back
kubectl rollout undo deployment app-cache --to-revision=1

# Scaling Workloads
## Manually
kubectl scale deployment app-cache --replicas=6
kubectl scale statefullset redis --replicas=4

## HPA (Horizontal Pod AutoSacaler)
kubectl autoscale deployment app-cache --cpu-percent=80 --min=3 --max=5

Warning :
The HorizontalPodAutoscaler is an API kind in the Kubernetes autoscaling API group. 
The current stable version can be found in the autoscaling/v2 API version which includes support for scaling on memory and custom metrics.

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: null
  name: app-cache
spec:
  maxReplicas: 3
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app-cache
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
  - type: Resource
    resource:
      name: memory
      target:
        type: AverageValue
        averageValue: 500Mi

For the metrics to appear when running kubectl get hpa, metric server should be installed and rsource request/limits should be configured.

# SECRET AND CONFIGMAP

#### CONFIGMAP
Source options for data parsed by a ConfigMap :

--from-literal : --from-literal=locale=en_US Literal values, which are key-value pairs as plain text
kubectl create configmap db-config --from-literal=DB_HOST=mysql-service --from-literal=DB_USER=backend
```
apiVersion: v1
data:
  DB_HOST: mysql-service
  DB_USER: backend
kind: ConfigMap
metadata:
  name: db-config
---
apiVersion: v1
kind: Pod
metadata:
  name: backend
spec:
  containers:
  - image: mysql:9
    name: backend
    envFrom:
    - configMapRef:
        name: db-config
```
--from-file : --from-file=app-config.json A file with arbitrary contents
kubectl create configmap db-config --from-file=db.json
```
apiVersion: v1
data:
  db-config.json: |
    {
     "db": {
     "host": "mysql-service",
     "user": "backend"
     }
    }
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: db-config2
---
apiVersion: v1
kind: Pod
metadata:
  name: db
spec:
  containers:
  - image: bmuschko/web-app:1.0.1
    name: db
    volumeMounts:
      - name: db-volume
        mountPath: /etc/config
  volumes:
    - name: db-volume
      configMap:
        name: db-config2
```

```
spec:
  containers:
  - envFrom:
      - configMapRef:
          name: webapp-color
    image: kodekloud/webapp-color
    imagePullPolicy: Always
    name: webapp-color
    resources: {}
```


--from-env-file : --from-env-file=config.env A file that contains key-value pairs and expects them to be environment variables
--from-file :  --from-file=config-dir A directory with one or many files

##### SECRET
--fromliteral : --from-literal=password=secret Literal values, which are key-value pairs as plain text

```
apiVersion: v1
kind: Pod
metadata:
  name: backend
spec:
  containers:
  - image: mysql:9
    name: backend
    envFrom:
    - configMapRef:
        name: db-config
```

--from-envfile : --from-env-file=config.env A file that contains key-value pairs and expects them to be environment variables
--from-file : --from-file=id_rsa=~/.ssh/id_rsa A file with arbitrary contents
--from-file : --from-file=config-dir A directory with one or many files

# Kustomize
Kustomize needs the kustomization.yaml and this file defines the processing rules Kustomize works upon.
kubectl kustomize <target> : similar to dry-run
kubectl apply -k <target> : To apply

```
configMapGenerator:
- name: db-config
  files:
   - config/db-config.properties
secretGenerator:
- name: db-creds
  files:
   - config/db-secret.properties
resources:
- web-app-pod.yaml
```
---

```
namespace: persistence
commonLabels:
 team: helix
resources:
- web-app-deployment.yaml
- web-app-service.yaml

```

```
resources:
- nginx-deployment.yaml
patchesStrategicMerge:
- security-context.yaml
```


############# udemy ##########
```
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - db/
  - monitoring/
  - nginx/

commonLabels:
  sandbox: dev
namespace: logging
images:
- name: nginx
  newName: httpd
  newTag: "2.4.6"
```

# patches with kustomize : Json 6902 vs Strategic Merge Patch
## Json 6902 Patch

```
patches:
  - target : 
      kind: Deployment
      name: api-deployment
    patch: |-
      - op: replace
        path: /sepc/replicas
        value: 5
        
```

## Strategic Merge Patch : Just copy the section you want to patch from a normal manifest

```
pacthes:
   - patch: |-
       apiVersion: apps/v1
       kind: Deployment
       metadata:
         name: api-deployment
       spec:
         replicas: 5
```
Or you can create a file like patch.yaml :

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-deployment
spec:
  replicas: 5
```

And call path.yaml in the kustomization.yaml

```
pacthes:
   - patch.yaml
```

```
resources:
  - mongo-depl.yaml
  - nginx-depl.yaml
  - mongo-service.yaml

patches:
  - target:
      kind: Deployment
      name: nginx-deployment
    patch: |-
      - op: replace
        path: /spec/replicas
        value: 3

  - target:
      kind: Deployment
      name: mongo-deployment
    path: mongo-label-patch.yaml

  - target:
      kind: Service
      name: mongo-cluster-ip-service
    patch: |-
      - op: replace
        path: /spec/ports/0/port
        value: 30000

      - op: replace
        path: /spec/ports/0/targetPort
        value: 30000
```

## delete a list

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-deployment
spec:
  template:
    spec:
      containers:
        - $patch: delete
          name: memcached
```
# Json6902
```
patches:
  - target:
      kind: deployment
      name: api-Deployment
    patch: |-
      op: remove
      path: /spec/template/spec/containers/1
  
```

### Overlays
### Components
When adding a new component entitity, do not add in resource, the path to the base. Or add in the overlays the patch to the component.


##### defines a security context on the container-level for the Pod template of the Deployment. The security rule should be define in a file like security-context.yaml


# YAML Processor yq
#### Reading values : yq -e
yq e .metadata.name pod.yaml

### Modifying values
yq e -i '.spec.containers[].env[1].value = "1.7.0"' spring-boot-app.yaml

# Helm
Chart.yaml describes the meta information of the chart (e.g., name and version)
values.yaml contains the key-value pairs used at runtime to replace the placeholders in the YAML manifests. 
helm template . : for templating, like dry-run
helm package : for tar the templating output

# Know the subnet assigned to the nodes by the CNI
kubectl get nodes k8s-worker-1 -o json | jq .spec.podCIDR

# Use busybox for testing
kubectl run busybox --image=busybox --rm -it --restart=Never -- wget 192.168.230.37:80

# Create service
k create service clusterip echoserver --tcp=80:8080 # create service that expose en existing pod pod echoserver
kubectl run echoserver1 --image=k8s.gcr.io/echoserver1:1.10 --restart=Never --port=8080 --expose # run a pod and expose it (service directly)
kubectl expose deployment echoserver --port=80 --target-port=8080 # expose a deployment

# create ingress
kubectl create ingress corellian  --rule="star-alliance.com/corellian/api=corellian:8080" # Where rule="<host>/<path>=<service>:<port>"
kubectl get ingress corellian --output=jsonpath="{.status.loadBalancer.ingress[0].ip}" # jsonpath to get the IP of an ingress IP

# Coredns
kubectl run busybox --image=busybox --rm -it --restart=Never -n dns -- wget echoserver:8080 # Make a call from a pod to a svc in the same ns using the hostname
k run busybox --image=busybox --rm -it --restart=Never -n business -- wget echoserver.other:8080 # From different ns
kubectl run busybox --image=busybox --rm -it --restart=Never -n business  -- wget echoserver.other.svc:8080
kubectl run busybox --image=busybox --rm -it --restart=Never -n business  -- wget echoserver.other.svc.cluster.local:8080
kubectl run busybox --image=busybox --rm -it --restart=Never -n ns1 -- wget 192-168-140-13.ns1.pod:8080 # make a call to a pod via the ip in the same ns. Where 192-168-140-13.ns1.pod:8080 is ip.ns.pod:port
Hierarchie : hostname(service_name/IP).namespace.type(svc/pod).root(cluster.local)
Ex : web-service.apps.svc.cluster.local | 10-244-2-5.apps.pod.cluster.local

#jsonPath
k get pod echoserver1 -n ns1 --template={{.status.podIP}} # get the IP of pod echoserver1 running in ns ns1

# Tshoot
## 503 Service Unavailable, service has no endpoint
Symptom: curl nginx.home.lan/echo returned 503 Service Temporarily Unavailable.
Cause: The Kubernetes Service echoserver had no active Endpoints (ENDPOINTS <none>). This was caused by a Label Selector Mismatch.
Service Selector: app=echoserver
Pod Label: run=echoserver
Because the labels didn't match, the Service didn't know which Pods to send traffic to.
The Fix: We edited the Service (kubectl edit svc) to match the Pod's label (run=echoserver).
Key Takeaway: A 503 from an Ingress usually means the Ingress Controller cannot connect to the backend Service. Always check kubectl get endpoints to verify the Service has found its Pods.


# Volume
## volume Types
emptyDir : Empty directory in Pod with read/write access. Persisted for only the lifespan of a Pod. A good choice for cache implementations or data exchange between containers of a Pod.
hostPath : File or directory from the host node’s filesystem.
configMap, secret: Provides a way to inject configuration data. For practical examples, see “Defining and Consuming Configuration Data”.
nfs : An existing Network File System (NFS) share. Preserves data after Pod restart.
persistent VolumeClaim : Claims a persistent volume.

## PVC/PV

┌──────────┐
│   Pod    │
│          │
│ utilise  │
│ un       │
│ volume   │
└────┬─────┘
     │
     ▼
┌───────────────┐
│      PVC      │
│ Persistent    │
│ Volume Claim  │
│               │
│ "J’ai besoin  │
│ de 10Gi,      │
│ RWX, etc."    │
└────┬──────────┘
     │
     ▼
┌───────────────┐
│      PV       │
│ Persistent    │
│ Volume        │
│               │
│ Stockage réel │
│ (NFS, disque, │
│ cloud, etc.)  │
└───────────────┘

## Volume Mode
Filesystem : Default. Mounts the volume into a directory of the consuming Pod. Creates a filesystem first if the volume is backed by a block device and the device is empty.
Block : Used for a volume as a raw block device without a filesystem on it.

## Access Mode
ReadWriteOnce | RWO : Read/write access by a single node
ReadOnlyMany | ROX : Read-only access by many nodes
ReadWriteMany | RWX : Read/write access by many nodes
ReadWriteOncePod | RWOP : Read/write access mounted by a single Pod
kubectl get pv db-pv -o jsonpath='{.spec.accessModes}' # get the access mode of a pv

## Claim policy
Retain : Default. When PersistentVolumeClaim is deleted, the PersistentVolume is “released” and can be reclaimed.
Delete : Deletion removes PersistentVolume and its associated storage
kubectl get pv db-pv -o jsonpath='{.spec.persistentVolumeReclaimPolicy}' # Retrieve the claim policy of a pv 

## Mounting PersistentVolumeClaims in a Pod : spec.volumes[].persistentVolumeClaim 
### Static provisioning 
#### create storage classes : spec.storageClassName 
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner 
volumeBindingMode: WaitForFirstConsumer

#### Create the PV
apiVersion: v1
kind: PersistentVolume
metadata:
 name: db-pv
spec:
 capacity:
   storage: 1Gi
 accessModes:
   - ReadWriteOnce
 hostPath:
   path: /data/db
 storageClassName: local-storage

#### Create the PVC
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
 name: test-pvc
spec:
 accessModes:
 - ReadWriteOnce
 resources:
   requests:
      storage: 512Mi

#### Mounting the PV in the pod by referencing the PVC # To retrive this shortly search for "Claims As Volumes" on k8s doc
apiVersion: v1
kind: Pod
metadata:
  name: app-consuming-pvc
spec:
  volumes:
    - name: app-storage
      persistentVolumeClaim:
        claimName: db-pvc
  containers:
    - name: app
      image: alpine
      command: ["/bin/sh"]
      args: ["-c", "while true; do sleep 60; done;"]
      volumeMounts:
        - name: app-storage
          mountPath: "/mnt/data"

### Autoprovisionnig : You need a provisionner that have this capacity. For our lab, we use Rancher Local Path Provisioner
Installation : kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
### Test :

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-local-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-path  # <--- This is the magic key
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
  - name: test-container
    image: busybox
    command: ["/bin/sh", "-c", "while true; do echo $(date) >> /data/test.txt; sleep 5; done"]
    volumeMounts:
    - mountPath: /data
      name: test-vol
  volumes:
  - name: test-vol
    persistentVolumeClaim:
      claimName: test-local-pvc
```

# Check if any pod is using the PVC
kubectl get pods --all-namespaces -o=jsonpath='{range .items[*]}{"\n"}{.metadata.name}{":\t"}{range .spec.volumes[*]}{.persistentVolumeClaim.claimName}{" "}{end}{end}' | grep <your-pvc-name>

# patch a PVC to remove the finalizer blocking a deletion. It works also for a PV
kubectl patch pvc <pvc-name> -n <namespace> -p '{"metadata":{"finalizers":null}}'

# Manual scheduling

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    resources: {}
  nodeName: k8s-worker-1
status: {}

# Labels and selectors
k get po --show-labels
k get po --selector app=App1
k get all --selector env=prod # Get all the object with label env-prod
k get pod --selector env=prod,bu=finance,tier=frontend # get pod part of many labels
k get po --selector app=echoserver2 --no-headers # get the pods whith label app=echoserver2, no-headers

# Taint and toleration : taint are set on node and toleration on pods
k taint nodes node-name key=value:taint-effect # taint-effect : NoSchedule, PreferNoSchedule, NoExecute
add a taint : k taint nodes node1 app=myapp:NoSchedule
Delete a taint :  k taint node controlplane node-role.kubernetes.io/control-plane:NoSchedule- 
### Add teleration in a pod. Should be align with containers section in the yaml file
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"

# Node selector
Label a node : k label node node-1 key=value

Then use nodeSelector to reference the label in the pod.
```
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    key: value
```
# Node affinity
requiredDuringSchedulingIgnoredDuringExecution: The scheduler can't schedule the Pod unless the rule is met.
preferredDuringSchedulingIgnoredDuringExecution: The scheduler tries to find a node that meets the rule. If a matching node is not available, the scheduler still schedules the Pod.
Operators : In, NotIn, Exists, DoesNotExist, Gt and Lt

# Static Pods
paste the pod yaml file in /etc/kubernetes/manifests/

# Get pririty classes on pods
kubectl get pods -o custom-columns="NAME:.metadata.name,PRIORITY:.spec.priorityClassName"

# Admission controller
check enabling plugins : ps -ef | grep kube-apiserver | grep admission-plugins
add plugins, edit the kube-apiserver manifest, look for the enable-admission-plugins section, and add the desired adminsion controller.
check the evailable plugins : k -n kube-system exec -it kube-apiserver-k8s-master -- kube-apiserver -h | grep enable-admission-plugins
MutatingAdmissionController are those that can change/mutate a requests 
ValidatingAdmissionController are those that can validate a request and allow or deny a request.
MutatingAdmissionWebhook : 
ValidatingAdmissionWebhook : 

# Command and args

apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - sleep
      - "2000"
      
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "pink"]

# Get secret in etcd db
etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key
 get /registry/secrets/apps/db-creds | hexdump -C

# mulicontainers
## initcontainers : Init containers always run to completion., Each init container must complete successfully before the next one starts.

```
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app.kubernetes.io/name: MyApp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"]
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"]
```

## sidecar container : sidecar are a specific case of initcontainers and  remain running after Pod startup.
Sidecacr container are used to enhance or to extend the functionality of the primary app container by providing additional services, or functionality such as logging, monitoring, security, or data synchronization, without directly altering the primary application code.
Sidecar containers are creating the same like initcontainers.The main difference is to set "restartPolicy: Always"

# In-Place Pod sizing : in k8s by default since k8s 1.27

# vpa
git clone https://github.com/kubernetes/autoscaler.git

---
apiVersion: "autoscaling.k8s.io/v1"
kind: VerticalPodAutoscaler
metadata:
  name: flask-app
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: flask-app-4
  updatePolicy:
    updateMode: "Off"  # You can set this to "Auto" if you want automatic updates
  resourcePolicy:
    containerPolicies:
      - containerName: '*'
        minAllowed:
          cpu: 100m
        maxAllowed:
          cpu: 1000m
        controlledResources: ["cpu"]
```

# Pod eviction timeout
kube-controller-manger --pod-eviction-timeout=5m0s
k drain node-1 # gracefully terminate the pods 
k uncordon node-1 # after a gracefull termination, uncordon the node for the node to be schedulable again
k cordon node-1 # mark a node unschedulable
kubectl drain node01 --ignore-daemonsets # force to drain all the pods in a node even daemonSets

# Security
## Authentication
- Generate private key : openssl genrsa -out my-bank.key 2048
- Generate pub key : openssl rsa -in my-bank.key -pubout > mybank.pem
- Generate a csr : openssl req -new -key my-bank.key -out my-bank.csr -subj "/C=US/ST=CA/O=MyOrg, Inc./CN=mybank.com"

# check certificate 
openssl x509 in /etc/kubernetes/pki/apiserver.crt -text -noout

# approve.sh
#!/usr/bin/env bash
kubectl certificate approve --kubeconfig admin.kubeconfig $(kubectl get csr --kubeconfig admin.kubeconfig -o json | jq -r '.items | .[]  | select(.spec.username == "system:node:node02") | .metadata.name')

# context
kubectl config --kubeconfig=/root/my-kube-config use-context research
kubectl config --kubeconfig=/root/my-kube-config current-context 

# Change the .kube/config file for our own one
Open your shell configuration file:
vi ~/.bashrc

Add one of these lines to export the variable:
export KUBECONFIG=/root/my-kube-config
# OR
export KUBECONFIG=~/my-kube-config
# OR
export KUBECONFIG=$HOME/my-kube-config

Apply the Changes:

Reload the shell configuration to apply the changes in the current session:

source ~/.bashrc

# Get all the clusterrole in a cluster
kubectl get clusterroles --no-headers  | wc -l 
kubectl get clusterroles --no-headers  -o json | jq '.items | length'

# Service accounts
k create tokent serviceaccount_name --duration 2h
kubectl set serviceaccount deploy/web-dashboard dashboard-sa

# decode a token
jq -R 'split(".") | select(length > 0) | .[0],.[1] | @base64d | fromjson' <<< paste the tocken 

# image secret 
# First create the secret
kubectl create secret docker-registry regcred --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>

Then add it in the pod.
```
apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: <your-private-image>
  imagePullSecrets:
  - name: regcred

```

# security contexts

```
apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo-4
spec:
  containers:
  - name: sec-ctx-4
    image: gcr.io/google-samples/hello-app:2.0
    securityContext:
      runAsUser: 1000
      runAsGroup: 3000
      fsGroup: 2000
      capabilities:
        add: ["NET_ADMIN", "SYS_TIME"]
```

```
apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  securityContext:
    runAsUser: 1001
  containers:
  -  image: ubuntu
     name: web
     command: ["sleep", "5000"]
     securityContext:
      runAsUser: 1002

  -  image: ubuntu
     name: sidecar
     command: ["sleep", "5000"]
```

Note: Ensure that you set the capabilities at the container level rather than at the pod level, as configuring them at the pod level is not supported.

# Network Policy : "s'exercer un peu plus sur les network security avant le test"

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-egress-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080
  - to:
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53

# kubectx and kubens 

kubectx : commands to switch between contexts

Installation:
sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx

Syntax:
To list all contexts: kubectx
To switch to a new context: kubectx <context_name>
To switch back to previous context: kubectx -
To see current context: kubectx -c

Kubens: This tool allows users to switch between namespaces quickly with a simple command.

Installation:
sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens

Syntax:
To switch to a new namespace: kubens <new_namespace>
To switch back to previous namespace: kubens -

# CRDs
spec.names.plural+"."+spec.group

cdr definistion :

```
--
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name:  internals.datasets.kodekloud.com
spec:
  group: datasets.kodekloud.com
  scope: Namespaced
  names:
    plural: internals
    singular: internal
    kind: Internal
    shortNames:
    - int
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                internalLoad:
                  type: string
                range:
                  type: integer
                percentage:
                  type: string
```

Object of the crd :

``` 
---
kind: Internal
apiVersion: datasets.kodekloud.com/v1
metadata:
  name: internal-space
  namespace: default
spec:
  internalLoad: "high"
  range: 80
  percentage: "50"
```


# Networking
cat /proc/sys/net/ipv4/ip_forward # enable forwarding hepending 1 in this file
netstat -anp | grep etcd | grep 2379 | wc -l

2379 : ETCD port to which all control plane components connect to
2380 : tcd peer-to-peer connectivity. When you have multiple controlplane nodes. 

# Ingress : 
- --default-backend-service=app-space/default-backend-service | image : kodekloud/ecommerce:404

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: / # really important do not forget it
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
  creationTimestamp: "2026-02-01T15:56:24Z"
  generation: 1
  name: ingress-wear-watch
  namespace: app-space
  resourceVersion: "1434"
  uid: 5c7885e3-3a27-4671-adf2-cbb3730611d9
spec:
  rules:
  - http:
      paths:
      - backend:
          service:
            name: wear-service
            port:
              number: 8080
        path: /wear
        pathType: Prefix
      - backend:
          service:
            name: video-service
            port:
              number: 8080
        path: /stream
        pathType: Prefix
status:
  loadBalancer:
    ingress:
    - ip: 172.20.73.45
```

# Rewrite 
When user visits the URL on the left, his request should be forwarded internally to the URL on the right. Note that the /watch and /wear URL path are what we configure on the ingress controller so we can forwarded users to the appropriate application in the backend. The applications don't have this URL/Path configured on them:

http://<ingress-service>:<ingress-port>/watch --> http://<watch-service>:<port>/

http://<ingress-service>:<ingress-port>/wear --> http://<wear-service>:<port>/

Without the rewrite-target option, this is what would happen:

http://<ingress-service>:<ingress-port>/watch --> http://<watch-service>:<port>/watch

http://<ingress-service>:<ingress-port>/wear --> http://<wear-service>:<port>/wear

Ex : 

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /pay
        backend:
          serviceName: pay-service
          servicePort: 8282
```

# GatewayAPI

- Installation :
kubectl kustomize "https://github.com/nginx/nginx-gateway-fabric/config/crd/gateway-api/standard?ref=v1.6.2" | kubectl apply -f -
kubectl kustomize "https://github.com/nginx/nginx-gateway-fabric/config/crd/gateway-api/experimental?ref=v1.6.2" | kubectl apply -f -
helm install ngf oci://ghcr.io/nginx/charts/nginx-gateway-fabric --create-namespace -n nginx-gateway

- GatewayClass : A GatewayClass defines a set of Gateways that are implemented by a specific controller. Think of it as a blueprint that tells Kubernetes which controller will manage the Gateways.

```
apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: nginx
spec:
  controllerName: nginx.org/gateway-controller
```
P.S : controllerName: This must match the name expected by your controller (e.g., nginx.org/gateway-controller for NGINX). It tells Kubernetes which controller will manage Gateways of this class.

- Configuring HTTP Gateway and Listener : A Gateway is a Kubernetes resource that defines how traffic enters your cluster.

```
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: nginx-gateway
  namespace: default
spec:
  gatewayClassName: nginx
  listeners:
  - name: http
    protocol: HTTP
    port: 80
    allowedRoutes:
      namespaces:
        from: All
```
- HTTPRoute : An HTTPRoute defines how HTTP traffic is forwarded to Kubernetes services. It works in conjunction with a Gateway to route requests based on specific rules, such as matching paths or header

```
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: basic-route
  namespace: default
spec:
  parentRefs:
  - name: nginx-gateway
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /app
    backendRefs:
    - name: my-app
      port: 80
```

-  HTTP Redirects and Rewrites : Redirects and rewrites are powerful tools for modifying incoming requests before they reach the backend service.

```
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: https-redirect
  namespace: default
spec:
  parentRefs:
  - name: nginx-gateway
  rules:
  - filters:
    - type: RequestRedirect
      requestRedirect:
        scheme: https
```
# Rewrites modify the request path before forwarding it to the backend.
```
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: rewrite-path
  namespace: default
spec:
  parentRefs:
  - name: nginx-gateway
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /old
    filters:
    - type: URLRewrite
      urlRewrite:
        path:
          replacePrefixMatch: /new
    backendRefs:
    - name: my-app
      port: 80
```

P.S : When creating HTTPRoute object in parente section, do not forget to specify the gateway namespace in case th gateway and the HTTPRoute object are note in the same ns.

# kubeadm
ps -p 1

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward=1
EOF

#Apply sysctl params without reboot
sudo sysctl --system

#verify that net.ipv4.ip_forward is set to 1 with:
sysctl net.ipv4.ip_forward

# To see the new version on ubuntu
sudo apt-cache madison kubeadm


# Helm : a pkg manager for k8s
Charts : a collection of file
Release : a single installation of an application using helm Charts. Within each release there are a revision
Revision : is like a snap of an apllication
values.yaml : is like a param file

## install Chart
helm install [release-name] [chart-name]
Ex : helm install my-site bitnami/wordpress 
apiVersion : v1 for helm 2 and v2 for helm 3
appVersion : version of the app
version : version of the chart

helm install myapp bitnami/nginx # from repo
helm install myapp ./mon-chart/ # from local
helm install myapp oci://ghcr.io/org/charts/myapp --version 1.2.3 # from oci repo
helm install myapp bitnami/nginx \
  -n production --create-namespace \
  -f values-prod.yaml \
  --set replicaCount=3 #create ns and set values with charts from repo
helm install myapp ./chart \
  --atomic \
  --timeout 5m \
  --wait # wait for the charts to be install

# add a helm repo and install a charts
helm repo add bitnami https://charts.bitnami.com/bitnami
helm install my-realease bitnami/wordpress 

# uninstall a release 
helm unistall my-release
helm uninstall myapp -n production
helm uninstall myapp --keep-history

# update a repo
helm repo update 

## pull and untar a chart
helm pull --untar [chart-name]

# Deploy the Apache application on the cluster using the apache from the bitnami repository. Set the release Name to: amaze-surf
helm install amaze-surf bitnami/apache

# upgrade a chart
helm upgrade release-name chart-name
helm upgrade dazzling-web bitnami/nginx --version 18.3.6
helm upgrade --install myapp ./chart \
  -n production \
  -f values-prod.yaml \
  --atomic \
  --wait

# check history
helm history release-name 

# rollback
helm rollback release-name revision-number

# search a chart in repo
helm search repo [your_search_string]

# helm template 
helm template myapp ./chart -f values.yaml # Voir ce que Helm va générer
helm template myapp ./chart | grep -A 50 "kind: Deployment" # Filtrer un type de ressource
helm template myapp ./chart -f values-prod.yaml > manifests.yaml # Sauvegarder pour inspection
helm template myapp ./chart --validate # Avec validation serveur (dry-run)

# TSHOOT
## Tshoot an app
- check the api : curl store.home.lab
- check the svc : curl the svc from a pod, describe the svc
- Does the svc exposed a pod ?
- check the labels of the pod and compare the one in the svc
- check the logs of the pods 
- If a svc is not reachable : describe the svc and his pods then compare, the slectors, the ports and the IPs (endpoints)
- "command failed" err="unable to load client CA provider: open /etc/kubernetes/pki/ca.crt: no such file or directory" : This kind of error does not means that the directory itself does not exist on the controller,
It may means that the directory is not mounted in the pod.



ImagePullBackOff or ErrImagePull :: Image could not be pulled from registry. : Check correct image name, check that image name exists in registry, verify network access from node to registry, ensure
proper authentication.
CrashLoopBackOff :: Application or command run in container crashes. : Check command executed in container, ensure that image can properly execute (e.g., by creating a container with Docker).
CreateContainerConfigError :: ConfigMap or Secret referenced by container cannot be found. : Check correct name of the configuration object, verify the existence of the configuration object in the namespace.

## Lists the events across all Pods for a given namespace.
kubectl get events 

## logs from the previous instantiation of a container : helpful if the container has been restarted
k logs --previous pod-name

## show pods and services labels
root@k8s-master ~/exam # k get svc corellian --show-labels 
NAME        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE   LABELS
corellian   ClusterIP   10.101.250.103   <none>        8080/TCP   11d   app=corellian
root@k8s-master ~/exam # k get pod corellian --show-labels 
NAME        READY   STATUS    RESTARTS   AGE   LABELS
corellian   1/1     Running   0          11d   app=corellian

P.S : The svc targetPort and the pod containerPort should be the same

## control plane tshoot
kubectl cluster-info
kubectl cluster-info dump

## worker nodes
check kubelet : 
   - service systemd status
   - certificate vality : openssl x509 -in /var/lib/kubelet/pki/kubelet.crt -text
check kube-proxy
check resources available

# JsonPath

cat q9.json | jpath '$.prizes[?(@.year == 2014)].laureates[*].firstname'

[start:end:end]
 cat input.json | jpath '$[-1:]' : get last element of a list
 cat input.json | jpath '$[-3:]' : 3 dernier elements d'une list

k get nodes -o=jsonpath='{.items[*].metadata.name}{"\n"}{.items[*].status.capacity.cpu}' # \n : new line
k get nodes -o=jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.capacity.cpu}{"\n"}{end}' # {range .items[*]} : boucle for dans items
## custom column 
k get nodes -o=custom-columns=<COLUMN NAME>:<JSON PATH>
k get nodes -o=custom-columns=NAME:.metadata.name,CPU:.status.capacity.cpu
# List Services Sorted by Name
kubectl get services --sort-by=.metadata.name # usually check k8s doc search for kubectl Quick Reference
k config view --kubeconfig=/root/my-kube-config -o=jsonpath='{.contexts[*].context.user}'

#
kubectl -n kube-system get configmap kubeadm-config -o yaml | grep podSubnet

# Generate a **Helm template** from the Argo CD chart **version `7.7.3`** for the **`argocd` namespace**
helm template argocd argo/argo-cd --version 7.7.3 --namespace argocd --set crds.install=false

--set crds.install=false  : Ensure that **CRDs are not installed** by configuring the chart accordingly


#Usefull cmd
k api-resources --namespaced=true -o name
kubectl logs -f kube-apiserver-cluster4-controlplane -n kube-system
kubectl get event --field-selector involvedObject.name=kube-apiserver-cluster4-controlplane -n kube-system
helm search hub nginx --list-repo-url

kubectl get event --field-selector involvedObject.name=<pod-name>


VJ : https://github.com/vj2201/CKA-PREP-2025-v2.git


annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"

# kkubectl explain : render the documentation of a k8s object

kubectl explain certificate.spec.subject --api-version=cert-manager.io/v1 > ~/subject.yaml