Note : 
Set auto-completion for aliases
complete -p YOUR_CMD # find the command fucntion
complete -F YOUR_CMD_FUNCTION kubectl # set this in bashrc

WARNING : FAIRE UN FOCUS SUR 
- LES API-GATEWAY
- LES STRATEGY DANS LES DEPLOYMENT
- KUSTOMIZE
- HELM
- YQ

## RBAC :
Subject: The user or process that wants to access a resource.
Resource: The Kubernetes API resource type e.g. a Deployment or node.
Verb: The operation that can be ex
kubectl create role <name> --verb=<verbs> --resource=<resources> -n <ns>
kubectl create rolebinding <name> --role=<role> --user=<user> -n <ns>
kubectl create clusterrole ...
kubectl create clusterrolebinding ...
kubectl auth can-i <verb> <resource> --as <user>


# create a user in k8s : https://kubernetes.io/docs/tasks/tls/certificate-issue-client-csr
# change the default ns
kubectl config set-context --current --namespace=NAMESPACE

# YAML syntax
microservice : #objet
  app : auth # key : value
  port : 300
  version : 1.7

# In case of list :

microservice :
  - app : auth
    port : 300
    version : 1.7
  - app: user
    port : 3009
    version : 1.8

# paste un multiline dans un yaml file
mylines: |
   line 1
   line 2
   line 3

Exemple executer un script 
command: 
   - sh
   - -C 
   - |
    Paste all 
    Your script 

#Paste a long sentence in  yaml
mylongsentence: >
    Mariame joue
    a la balle 
    avec bile 
    son ami

# check permission
kubectl auth can-i --list --as johndoe
kubectl auth can-i list pods --as johndoe

# Update iptables config
cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sudo sysctl --system

# Backup and restore ETCD
### Backup
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=<trusted-ca-file> --cert=<cert-file> --key=<key-file> \
  snapshot save <backup-file-location>

Exemple : 
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /tmp/snapshot.db

#### Restore
export ETCDCTL_API=3
etcdctl --data-dir <data-dir-location> snapshot restore snapshot.db

Now edit /etc/kubernetes/manifests/etcd.yaml and change **volumes.hostPath.path** for name: etcd-data to <data-dir-location>,
then execute kubectl -n kube-system delete pod <name-of-etcd-pod> or systemctl restart kubelet.service or both

# Determine the api-server endpoint
kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}'

# Get the Secret access token of a ServiceAccount
kubectl get secret $(kubectl get serviceaccount api-access -n apps \
 -o jsonpath='{.secrets[0].name}') -o jsonpath='{.data.token}' -n apps \
 | base64 --decode

# Curl for listing pods in ns rm from a pod
curl https://10.10.10.38:6443/api/v1/namespaces/rm/pods --header --insecure

#  Rolling Updates and Rollbacks 
### Rolling update
kubectl set image deployment app-cache memcached=memcached:1.6.10 --record

### Check a rollout status
k rollout status deployment app-cache

### Check rollout history
k rollout history deployment app-cache

### Get more details about a rollout
k rollout history deployment app-cache --revision=2

### Rolling back
kubectl rollout undo deployment app-cache --to-revision=1

# Scaling Workloads
## Manually
kubectl scale deployment app-cache --replicas=6
kubectl scale statefullset redis --replicas=4

## HPA (Horizontal Pod AutoSacaler)
kubectl autoscale deployment app-cache --cpu-percent=80 --min=3 --max=5

Warning :
The HorizontalPodAutoscaler is an API kind in the Kubernetes autoscaling API group. 
The current stable version can be found in the autoscaling/v2 API version which includes support for scaling on memory and custom metrics.

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: null
  name: app-cache
spec:
  maxReplicas: 3
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app-cache
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
  - type: Resource
    resource:
      name: memory
      target:
        type: AverageValue
        averageValue: 500Mi

For the metrics to appear when running kubectl get hpa, metric server should be installed and rsource request/limits should be configured.

# SECRET AND CONFIGMAP

#### CONFIGMAP
Source options for data parsed by a ConfigMap :

--from-literal : --from-literal=locale=en_US Literal values, which are key-value pairs as plain text
kubectl create configmap db-config --from-literal=DB_HOST=mysql-service --from-literal=DB_USER=backend
```
apiVersion: v1
data:
  DB_HOST: mysql-service
  DB_USER: backend
kind: ConfigMap
metadata:
  name: db-config
---
apiVersion: v1
kind: Pod
metadata:
  name: backend
spec:
  containers:
  - image: mysql:9
    name: backend
    envFrom:
    - configMapRef:
        name: db-config
```
--from-file : --from-file=app-config.json A file with arbitrary contents
kubectl create configmap db-config --from-file=db.json
```
apiVersion: v1
data:
  db-config.json: |
    {
     "db": {
     "host": "mysql-service",
     "user": "backend"
     }
    }
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: db-config2
---
apiVersion: v1
kind: Pod
metadata:
  name: db
spec:
  containers:
  - image: bmuschko/web-app:1.0.1
    name: db
    volumeMounts:
      - name: db-volume
        mountPath: /etc/config
  volumes:
    - name: db-volume
      configMap:
        name: db-config2
```

--from-env-file : --from-env-file=config.env A file that contains key-value pairs and expects them to be environment variables
--from-file :  --from-file=config-dir A directory with one or many files

##### SECRET
--fromliteral : --from-literal=password=secret Literal values, which are key-value pairs as plain text

```
apiVersion: v1
kind: Pod
metadata:
  name: backend
spec:
  containers:
  - image: mysql:9
    name: backend
    envFrom:
    - configMapRef:
        name: db-config
```

--from-envfile : --from-env-file=config.env A file that contains key-value pairs and expects them to be environment variables
--from-file : --from-file=id_rsa=~/.ssh/id_rsa A file with arbitrary contents
--from-file : --from-file=config-dir A directory with one or many files

# Kustomize
Kustomize needs the kustomization.yaml and this file defines the processing rules Kustomize works upon.
kubectl kustomize <target> : similar to dry-run
kubectl apply -k <target> : To apply

```
configMapGenerator:
- name: db-config
  files:
   - config/db-config.properties
secretGenerator:
- name: db-creds
  files:
   - config/db-secret.properties
resources:
- web-app-pod.yaml
```
---

```
namespace: persistence
commonLabels:
 team: helix
resources:
- web-app-deployment.yaml
- web-app-service.yaml
```
---

##### defines a security context on the container-level for the Pod template of the Deployment. The security rule should be define in a file like security-context.yaml

```
resources:
- nginx-deployment.yaml
patchesStrategicMerge:
- security-context.yaml
```

# YAML Processor yq
#### Reading values : yq -e
yq e .metadata.name pod.yaml

### Modifying values
yq e -i '.spec.containers[].env[1].value = "1.7.0"' spring-boot-app.yaml

# Helm
Chart.yaml describes the meta information of the chart (e.g., name and version)
values.yaml contains the key-value pairs used at runtime to replace the placeholders in the YAML manifests. 
helm template . : for templating, like dry-run
helm package : for tar the templating output

# Know the subnet assigned to the nodes by the CNI
kubectl get nodes k8s-worker-1 -o json | jq .spec.podCIDR

# Use busybox for testing
kubectl run busybox --image=busybox --rm -it --restart=Never -- wget 192.168.230.37:80

# Create service
k create service clusterip echoserver --tcp=80:8080 # create service that expose en existing pod pod echoserver
kubectl run echoserver1 --image=k8s.gcr.io/echoserver1:1.10 --restart=Never --port=8080 --expose # run a pod and expose it (service directly)
kubectl expose deployment echoserver --port=80 --target-port=8080 # expose a deployment

# create ingress
kubectl create ingress corellian  --rule="star-alliance.com/corellian/api=corellian:8080" # Where rule="<host>/<path>=<service>:<port>"
kubectl get ingress corellian --output=jsonpath="{.status.loadBalancer.ingress[0].ip}" # jsonpath to get the IP of an ingress IP

# Coredns
kubectl run busybox --image=busybox --rm -it --restart=Never -n dns -- wget echoserver:8080 # Make a call from a pod to a svc in the same ns using the hostname
k run busybox --image=busybox --rm -it --restart=Never -n business -- wget echoserver.other:8080 # From different ns
kubectl run busybox --image=busybox --rm -it --restart=Never -n business  -- wget echoserver.other.svc:8080
kubectl run busybox --image=busybox --rm -it --restart=Never -n business  -- wget echoserver.other.svc.cluster.local:8080
kubectl run busybox --image=busybox --rm -it --restart=Never -n ns1 -- wget 192-168-140-13.ns1.pod:8080 # make a call to a pod via the ip in the same ns. Where 192-168-140-13.ns1.pod:8080 is ip.ns.pod:port

#jsonPath
k get pod echoserver1 -n ns1 --template={{.status.podIP}} # get the IP of pod echoserver1 running in ns ns1


# Tshoot
## 503 Service Unavailable, service has no endpoint
Symptom: curl nginx.home.lan/echo returned 503 Service Temporarily Unavailable.
Cause: The Kubernetes Service echoserver had no active Endpoints (ENDPOINTS <none>). This was caused by a Label Selector Mismatch.
Service Selector: app=echoserver
Pod Label: run=echoserver
Because the labels didn't match, the Service didn't know which Pods to send traffic to.
The Fix: We edited the Service (kubectl edit svc) to match the Pod's label (run=echoserver).
Key Takeaway: A 503 from an Ingress usually means the Ingress Controller cannot connect to the backend Service. Always check kubectl get endpoints to verify the Service has found its Pods.


# Volume
## volume Types
emptyDir : Empty directory in Pod with read/write access. Persisted for only the lifespan of a Pod. A good choice for cache implementations or data exchange between containers of a Pod.
hostPath : File or directory from the host node’s filesystem.
configMap, secret: Provides a way to inject configuration data. For practical examples, see “Defining and Consuming Configuration Data”.
nfs : An existing Network File System (NFS) share. Preserves data after Pod restart.
persistent VolumeClaim : Claims a persistent volume.

## PVC/PV

┌──────────┐
│   Pod    │
│          │
│ utilise  │
│ un       │
│ volume   │
└────┬─────┘
     │
     ▼
┌───────────────┐
│      PVC      │
│ Persistent    │
│ Volume Claim  │
│               │
│ "J’ai besoin  │
│ de 10Gi,      │
│ RWX, etc."    │
└────┬──────────┘
     │
     ▼
┌───────────────┐
│      PV       │
│ Persistent    │
│ Volume        │
│               │
│ Stockage réel │
│ (NFS, disque, │
│ cloud, etc.)  │
└───────────────┘

## Volume Mode
Filesystem : Default. Mounts the volume into a directory of the consuming Pod. Creates a filesystem first if the volume is backed by a block device and the device is empty.
Block : Used for a volume as a raw block device without a filesystem on it.

## Access Mode
ReadWriteOnce | RWO : Read/write access by a single node
ReadOnlyMany | ROX : Read-only access by many nodes
ReadWriteMany | RWX : Read/write access by many nodes
ReadWriteOncePod | RWOP : Read/write access mounted by a single Pod
kubectl get pv db-pv -o jsonpath='{.spec.accessModes}' # get the access mode of a pv

## Claim policy
Retain : Default. When PersistentVolumeClaim is deleted, the PersistentVolume is “released” and can be reclaimed.
Delete : Deletion removes PersistentVolume and its associated storage
kubectl get pv db-pv -o jsonpath='{.spec.persistentVolumeReclaimPolicy}' # Retrieve the claim policy of a pv 

## Mounting PersistentVolumeClaims in a Pod : spec.volumes[].persistentVolumeClaim 
### Static provisioning 
#### create storage classes : spec.storageClassName 
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner 
volumeBindingMode: WaitForFirstConsumer

#### Create the PV
apiVersion: v1
kind: PersistentVolume
metadata:
 name: db-pv
spec:
 capacity:
   storage: 1Gi
 accessModes:
   - ReadWriteOnce
 hostPath:
   path: /data/db
 storageClassName: local-storage

#### Create the PVC
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
 name: test-pvc
spec:
 accessModes:
 - ReadWriteOnce
 resources:
   requests:
      storage: 512Mi

#### Mounting the PV in the pod by referencing the PVC # To retrive this shortly search for "Claims As Volumes" on k8s doc
apiVersion: v1
kind: Pod
metadata:
  name: app-consuming-pvc
spec:
  volumes:
    - name: app-storage
      persistentVolumeClaim:
        claimName: db-pvc
  containers:
    - name: app
      image: alpine
      command: ["/bin/sh"]
      args: ["-c", "while true; do sleep 60; done;"]
      volumeMounts:
        - name: app-storage
          mountPath: "/mnt/data"

### Autoprovisionnig : You need a provisionner that have this capacity. For our lab, we use Rancher Local Path Provisioner
Installation : kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
### Test :

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-local-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-path  # <--- This is the magic key
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
  - name: test-container
    image: busybox
    command: ["/bin/sh", "-c", "while true; do echo $(date) >> /data/test.txt; sleep 5; done"]
    volumeMounts:
    - mountPath: /data
      name: test-vol
  volumes:
  - name: test-vol
    persistentVolumeClaim:
      claimName: test-local-pvc
```

# Check if any pod is using the PVC
kubectl get pods --all-namespaces -o=jsonpath='{range .items[*]}{"\n"}{.metadata.name}{":\t"}{range .spec.volumes[*]}{.persistentVolumeClaim.claimName}{" "}{end}{end}' | grep <your-pvc-name>

# patch a PVC to remove the finalizer blocking a deletion. It works also for a PV
kubectl patch pvc <pvc-name> -n <namespace> -p '{"metadata":{"finalizers":null}}'

# Tshoot

ImagePullBackOff or ErrImagePull :: Image could not be pulled from registry. : Check correct image name, check that image name exists in registry, verify network access from node to registry, ensure
proper authentication.
CrashLoopBackOff :: Application or command run in container crashes. : Check command executed in container, ensure that image can properly execute (e.g., by creating a container with Docker).
CreateContainerConfigError :: ConfigMap or Secret referenced by container cannot be found. : Check correct name of the configuration object, verify the existence of the configuration object in the namespace.

## Lists the events across all Pods for a given namespace.
kubectl get events 

## logs from the previous instantiation of a container : helpful if the container has been restarted
k logs --previous pod-name

## show pods and services labels
root@k8s-master ~/exam # k get svc corellian --show-labels 
NAME        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE   LABELS
corellian   ClusterIP   10.101.250.103   <none>        8080/TCP   11d   app=corellian
root@k8s-master ~/exam # k get pod corellian --show-labels 
NAME        READY   STATUS    RESTARTS   AGE   LABELS
corellian   1/1     Running   0          11d   app=corellian

P.S : The svc targetPort and the pod containerPort should be the same

## control plane tshoot
kubectl cluster-info
kubectl cluster-info dump

## worker nodes
check kubelet : 
   - service systemd status
   - certificate vality : openssl x509 -in /var/lib/kubelet/pki/kubelet.crt -text
check kube-proxy
check resources available

# Manual scheduling

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    resources: {}
  nodeName: k8s-worker-1
status: {}

# Labels and selectors
k get po --show-labels
k get po --selector app=App1
k get all --selector env=prod # Get all the object with label env-prod
k get pod --selector env=prod,bu=finance,tier=frontend # get pod part of many labels
k get po --selector app=echoserver2 --no-headers # get the pods whith label app=echoserver2, no-headers

# Taint and toleration : taint are set on node and toleration on pods
k taint nodes node-name key=value:taint-effect # taint-effect : NoSchedule, PreferNoSchedule, NoExecute
add a taint : k taint nodes node1 app=myapp:NoSchedule
Delete a taint :  k taint node controlplane node-role.kubernetes.io/control-plane:NoSchedule- 
### Add teleration in a pod. Should be align with containers section in the yaml file
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"

# Node selector
Label a node : k label node node-1 key=value

Then use nodeSelector to reference the label in the pod.
```
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    key: value
```
# Node affinity
requiredDuringSchedulingIgnoredDuringExecution: The scheduler can't schedule the Pod unless the rule is met.
preferredDuringSchedulingIgnoredDuringExecution: The scheduler tries to find a node that meets the rule. If a matching node is not available, the scheduler still schedules the Pod.
Operators : In, NotIn, Exists, DoesNotExist, Gt and Lt

# Static Pods
paste the pod yaml file in /etc/kubernetes/manifests/

# Get pririty classes on pods
kubectl get pods -o custom-columns="NAME:.metadata.name,PRIORITY:.spec.priorityClassName"

# Admission controller
check enabling plugins : ps -ef | grep kube-apiserver | grep admission-plugins
add plugins, edit the kube-apiserver manifest, look for the enable-admission-plugins section, and add the desired adminsion controller.
check the evailable plugins : k -n kube-system exec -it kube-apiserver-k8s-master -- kube-apiserver -h | grep enable-admission-plugins